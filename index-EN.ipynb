{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "!jupyter nbconvert index.ipynb --to slides --reveal-prefix reveal.js --post serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>MINISTRY OF EDUCATION AND SCIENCE OF UKRAINE<br>\n",
    "NATIONAL TECHNICAL UNIVERSITY OF UKRAINE<br>\n",
    "\"IGOR SIKORSKY KYIV POLYTECHNIC INSTITUTE\"<br>\n",
    "INSTITUTE OF APPLIED SYSTEM ANALYSIS<br>\n",
    "DEPARTMENT OF MATHEMATICAL METHODS OF SYSTEM ANALYSIS</center>\n",
    "\n",
    "\n",
    "### Thesis on:\n",
    "# Application of Long short-term memory algorythm in human activity recognition problems\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: right\">\n",
    "Done by:<br>\n",
    "Valentyn Melnychuk, КА-41<br>\n",
    "Supervisor:<br>\n",
    "Ph.D., Associate Professor, Alla Yakovleva <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Object of study\n",
    "A deep learning algorithm - **long short-term memory (LSTM)** for classification of human activity.\n",
    "\n",
    "\n",
    "### Subject of study\n",
    "Application of the LSTM in three classification tasks:\n",
    "- classification of human activity by video (handclapping, handwaving, jogging, running, etc.)\n",
    "- classification of human activity by data from gyroscope and accelerometer (walking, walking upstairs, walking downstairs, etc.)\n",
    "- recognition of reading by the human gaze trajectory\n",
    "\n",
    "### Aim of the study\n",
    "- verification of the efficiency of the LSTM algorithm in tasks of activity classification\n",
    "- study of its modifications and combination with other methods of deep learning\n",
    "- stating advantages and disadvantages of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Formulation of the problem\n",
    "\n",
    "The classification model for sequence data is described as follows:\n",
    "<center>Input sequence $\\{x^{(t)} \\in \\mathbb{R}^n, t \\in (1,2 ... \\tau)\\}$ -> Category from some fixed set $\\hat{y} \\in (1,2 ... K) $<center/>\n",
    "<br>\n",
    "    \n",
    "One constructs a model that will estimate the dependence according to the given **quality criterion** based on the given sample:    \n",
    "    \n",
    "$$D = \\Big\\{(x_i, y_i); x_i = \\{x^{(t)}_i \\in \\mathbb{R}^n, t \\in (1,2 ... \\tau_i)\\}, \\\\ y_i \\in (1,2 ... K), i \\in (1, ..., m)\\Big\\}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1. KTH dataset\n",
    "\n",
    "KTH dataset was created by Schuldt et. al. in 2004 and is the most popular dataset for human activity recognition.\n",
    "\n",
    "Input data: \n",
    "- 600 videos of different length (100 for each category)\n",
    "- Video is a sequence of monochrome frames (matrices) of size 120*160 \n",
    "\n",
    "Output data:\n",
    "- 6 types of activity: 'boxing', 'handclapping', 'handwaving', 'jogging', 'running', 'walking'\n",
    "<br>\n",
    "<img src=\"actions.jpg\" width=\"700\">\n",
    "<center>Image 1. 6 types of activity</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2. HAR dataset\n",
    "\n",
    "The HAR dataset was built from records of 30 participants, which performed 6 daily activities by holding a smartphone on waist with integrated accelerometer and gyroscope\n",
    "\n",
    "Input data:\n",
    "- 7352 time series of length 128 time steps\n",
    "- Each time series - sequence of 9-dimensional vactors, which contain information about the acceleration and angular velocity\n",
    "\n",
    "Output data:\n",
    "\n",
    "- 6 types of activity: 'walking', 'walking upstairs', 'walking downstairs', 'sitting', 'standing', 'laying'\n",
    "<br>\n",
    "<img src=\"har_sample.png\" width=\"600\">\n",
    "<center>Image 2. Accelaration in 3 dimentions for 4 types of activity</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.3. GAZEPOINT dataset\n",
    "\n",
    "One used dataset generated by the Gazepoint gaze tracker. 15 participants took part in the dataset recording, each of whom had to read two texts, find a specific picture, search in the text and watch a video.\n",
    "\n",
    "Input data:\n",
    "- 143 sequences with the trajectory of human gaze, recorded with a frequency of 60 Hz \n",
    "- each sequence has following features: gaze coordinates on screen and duration of a fixation.\n",
    "\n",
    "Output data:\n",
    "\n",
    "- 2 types of activity: 'reading', 'non-reading'\n",
    "<br>\n",
    "<img src=\"gazepoint_sample_reading.png\" width=\"600\">\n",
    "<center>Image 3. Trajectory of gaze movement and duration of fixation for reading and non-reading</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Quality criterion for model selection\n",
    "\n",
    "As quality criterions $J(D, \\theta)$ for model with parameters $\\theta$ were chosen the following metrics:\n",
    "- average value of cost function  $L$ on validation subset: \n",
    "$$J (D, \\theta ) = \\frac{1}{m_{valid}} \\sum_{i=1}^{m_{valid}}{L(x_i, y_i)}$$\n",
    "- classification accuracy on validation subset:\n",
    "$$J (D, \\theta ) = \\frac{1}{m_{valid}} \\sum_{i=1}^{m_{valid}}{\\mathbb{I}_{y_i = \\tilde{y}_i}}$$\n",
    "- confusion matrix on validation subset:\n",
    "$$J (D, \\theta ) =  {\\vert\\vert \\sum_{i=1}^{m_{valid}}{\\mathbb{I}_{y_i = k, \\tilde{y}_j = l}} \\vert\\vert }_{k, l \\in (1, ..., K)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Overview of existing approaches to problem solving\n",
    "\n",
    "- Classical activity recognition approaches require **feature engineering**, since traditional **feed-forward neural networks** can not handle sequences of different lengths and do not have the **memory property**.\n",
    "\n",
    "- Recurent Neural Networks (RNNs), that contain **feedback-loop**, help to solve these disadvantages and allow to store information about the whole sequence of input data. The main problem of RNN - **vanishing gradient problem** and the usual RNN does not take into account long-term dependencies.\n",
    "\n",
    "- The LSTM partially solves this problem, since it recursively transfers the state without applying activation or multiplying it by the matrix of weights, using gates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.1. Recurent neural networks\n",
    "**Recurent neural network** - neural network, which implements the idea of **parameters sharing**, and is designed to work with sequences of arbitrary lengths $x^{(1)}, ..., x^{(\\tau)}$. In fact, it is nonlinear autoregressive exogenous model (NARX).\n",
    "\n",
    "Computational graph of vanilla RNN in unfolded view | Model equation \n",
    "- | - \n",
    "<img src=\"rnn.png\" style=\"width: 350px;\"> | <img src=\"rnn_formula.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.2. Types of Recurent neural networks\n",
    "\n",
    "There are three main architectures of the RNNs, depending on the task: \n",
    "\n",
    "Many-to-many | Many-to-one | One-to-many \n",
    "- | - \n",
    "<img src=\"rnn.png\" style=\"width: 300px;\"> | <img src=\"many_to_one.png\" style=\"width: 220px;\"> | <img src=\"one-to-many.png\" style=\"width: 270px;\">\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.3. Recurent neural networks in classification problems\n",
    "\n",
    "The most commonly used architecture is **many-to-one**, sometimes **many-to-many** (continious classification).\n",
    "\n",
    "Prediction of **many-to-one** model will be: $$\\hat{y} = softmax(o); \\quad softmax(z)_{i}={\\frac {e^{z_{i}}}{\\sum _{k=1}^{K}e^{z_{k}}}}, i = 1, ..., K$$\n",
    "\n",
    "Loss function $L$ for pair $(x, y)$ of input sequence and output category will be cross-entropy (negative log-likelihood):\n",
    "\n",
    "$$L(\\{x^{(1)}, ..., x^{(\\tau)}\\}, y \\}) = - \\sum_i{(y)_i * ln ((\\hat{y})_i)}$$\n",
    "where $\\hat{y}$ - predicted output value, $y$ - real output value in one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.4. Recurent neural networks in classification problems\n",
    "\n",
    "To find the weights of the model it is necessary to minimize the cost function:\n",
    "$$ J = \\frac{1}{m_{train}} \\sum_{i=1}^{m_{train}}{L(x_i, y_i)}$$\n",
    "\n",
    "Dependencies in RNN between $\\hat{y}$ and $\\{x^{(1)}, ..., x^{(\\tau)}\\}$ is described by differentiatable functions, thus we can use gradient-based methods.\n",
    "\n",
    "The **Adam optimizer**, which is a modified stochastic gradient descent, is used in the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.1. Long short-term memory\n",
    "\n",
    "Long short-term memory (LSTM) is the architecture of recurrent neural networks, proposed in 1997 by Zepp Hohreiter and Jürgen Schmidguber.\n",
    "\n",
    "The LSTM is specifically designed to avoid **vanishing gradient problem**, which is inherent in conventional RNNs.\n",
    "\n",
    "Vanilla LSTM is described by following equations:\n",
    "$$ \\begin{split}\n",
    "    f^{(t)}  & = \\sigma(W_f [h^{(t-1)}, x^{(t)}] + b_f)\\\\\n",
    "    i^{(t)} & = \\sigma(W_i  [h^{(t-1)}, x^{(t)}] + b_i) \\\\\n",
    " C^{(t)} & = f^{(t)} \\circ C^{(t-1)} + i^{(t)} \\circ tanh(W_C  [h^{(t-1)}, x^{(t)}] + b_C) \\\\\n",
    " \\tilde{h}^{(t)} & = \\sigma(W_o  [h^{(t-1)}, x^{(t)}] + b_o) \\\\\n",
    "  h^{(t)} & = \\tilde{h}^{(t)} \\circ tanh(C ^{ {(t)}})\n",
    " \\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.2. Computational graph of LSTM\n",
    "\n",
    "LSTM contains 4 recurrent layers and 3 gates: input, forgetting and output. \n",
    "\n",
    "<img src=\"lstm_new1.png\" width=\"600\">\n",
    "where rectangles are layers of network, $\\circ$ - Hadamard product.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8.1. KTH - results\n",
    "\n",
    "Data pre-processing:\n",
    "- Differentiation $x'^{(t)} = x^{(t)} - x^{(t-1)}$, to take into account only moving parts of frames and normalize the data\n",
    "- Cropping on subsequences of 25 frames\n",
    "\n",
    "<img src=\"kth_difference.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two models were developed:\n",
    "- CNN-LSTM - 3-dimensional CNN + 2-layers LSTM\n",
    "- CONV-LSTM - convolutional LSTM (3 layers)\n",
    "\n",
    "Results  |Confusion matrix of the best model\n",
    "- | - \n",
    "<img src=\"kth_result.png\" style=\"width: 450px;\"> | <img src=\"kth_confusion.png\" alt=\"Drawing\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<video src=\"kth_test.mp4\" width=\"600\" controls>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8.2. HAR - results\n",
    "\n",
    "Without data pre-processing.\n",
    "\n",
    "3 models were developed:\n",
    "- vanilla LSTM \n",
    "- LSTM with $L_2$ regularization\n",
    "- LSTM with dropout\n",
    "\n",
    "Results | Confusion matrix of the best model\n",
    "- | - \n",
    "<img src=\"har_result.png\" style=\"width: 450px;\"> | <img src=\"har_confusion.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8.3. GAZEPOINT - results\n",
    "\n",
    "Data pre-processing:\n",
    "- Differentiation of the trajectory of a gaze $x'^{(t)} = x^{(t)} - x^{(t-1)}$.\n",
    "\n",
    "4 models were developed:\n",
    "- Vanilla LSTM with 3-dimensional input sequence, 100 time steps\n",
    "- LSTM-reshape(6) with 6-dimensional input sequence, 50 time steps\n",
    "- LSTM-reshape(12) with 12-dimensional input sequence, 25 time steps\n",
    "- CNN-5 - 5-layers convolutional neural network (for comparison)\n",
    "\n",
    "<img src=\"gp_result.png\" style=\"width: 450px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 9. Dependency of the quality of the model on the length of the input sequence\n",
    " \n",
    "KTH  | HAR \n",
    "- | - \n",
    "<img src=\"kth_different.png\" alt=\"Drawing\" style=\"width: 450px;\"> | <img src=\"har_different.png\" alt=\"Drawing\" style=\"width: 450px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10. Further ideas for research\n",
    "\n",
    "- Training on sequences of different lengths (one must have a priori knowledge of the duration of the dependency).\n",
    "- Comparison of various LSTM architectures (peephole, with combined input and forgetting gates). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 11.1. Conclusions - advantages\n",
    "\n",
    "In this paper, research was conducted on the possibility and effectiveness of the use of the LSTMs for the problems of classification of human activity.\n",
    "\n",
    "Among the advantages of the LSTMs, one can mention the following:\n",
    "- No need in pre-processing\n",
    "- Scalability and universatility\n",
    "- Ability to work with different lengths of input sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 11.2. Conclusions - disadvantages\n",
    "\n",
    "In addition to the advantages of method listed above, one revealed some of its disadvantages:\n",
    "\n",
    "- The LSTM does not really require features engineering, but in some cases it is more accurate to build a model on input with pre-processing\n",
    "- The LSTM is not a universal algorithm for sequences; for example, it could not properly classify 3-dimensional sequences of GAZEPOINT dataset\n",
    "- It may sometimes be difficult to pick up the optimal model architecture\n",
    "- Fitting these models may take some time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks for the attetion!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
